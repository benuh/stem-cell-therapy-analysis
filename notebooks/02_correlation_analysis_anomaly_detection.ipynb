{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Correlation Analysis & Anomaly Detection\n",
    "\n",
    "## Autonomous Statistical Exploration Session\n",
    "\n",
    "**Objective**: Identify unusual correlations, behavioral patterns, and anomalies in stem cell therapy data without supervision  \n",
    "**Approach**: Comprehensive statistical exploration with visualization and significance testing  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import comprehensive analysis libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import pearsonr, spearmanr, kendalltau, chi2_contingency\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "import networkx as nx\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure enhanced plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"Set2\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "%matplotlib inline\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "df = pd.read_csv('../data/clinical_trial_data.csv')\n",
    "\n",
    "print(\"=== AUTONOMOUS DATA EXPLORATION INITIATED ===\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Features: {list(df.columns)}\")\n",
    "print(f\"Data types: {df.dtypes.value_counts()}\")\n",
    "\n",
    "# Create numerical encoding for categorical variables\n",
    "df_encoded = df.copy()\n",
    "le_dict = {}\n",
    "\n",
    "categorical_cols = ['condition', 'intervention', 'primary_endpoint', 'phase', 'status', 'country']\n",
    "for col in categorical_cols:\n",
    "    if col in df_encoded.columns:\n",
    "        le = LabelEncoder()\n",
    "        df_encoded[f'{col}_encoded'] = le.fit_transform(df_encoded[col].astype(str))\n",
    "        le_dict[col] = le\n",
    "\n",
    "# Select numerical features for correlation analysis\n",
    "numerical_features = ['n_patients', 'treatment_group', 'control_group', 'endpoint_value', \n",
    "                     'baseline_value', 'percent_change', 'follow_up_months', 'safety_events']\n",
    "encoded_features = [col for col in df_encoded.columns if col.endswith('_encoded')]\n",
    "all_features = numerical_features + encoded_features\n",
    "\n",
    "# Create analysis dataframe with only complete cases for correlation analysis\n",
    "analysis_df = df_encoded[all_features].copy()\n",
    "print(f\"\\nFeatures for analysis: {len(all_features)}\")\n",
    "print(f\"Complete cases: {analysis_df.dropna().shape[0]}/{len(analysis_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Comprehensive Correlation Matrix Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate multiple correlation metrics\n",
    "def calculate_comprehensive_correlations(df):\n",
    "    \"\"\"\n",
    "    Calculate Pearson, Spearman, and Kendall correlations\n",
    "    \"\"\"\n",
    "    correlations = {}\n",
    "    \n",
    "    # Remove rows with any NaN values for correlation analysis\n",
    "    df_clean = df.dropna()\n",
    "    \n",
    "    if len(df_clean) < 3:\n",
    "        print(\"Insufficient data for correlation analysis\")\n",
    "        return correlations\n",
    "    \n",
    "    # Pearson correlation (linear relationships)\n",
    "    correlations['pearson'] = df_clean.corr(method='pearson')\n",
    "    \n",
    "    # Spearman correlation (monotonic relationships)\n",
    "    correlations['spearman'] = df_clean.corr(method='spearman')\n",
    "    \n",
    "    # Kendall correlation (ordinal relationships)\n",
    "    correlations['kendall'] = df_clean.corr(method='kendall')\n",
    "    \n",
    "    return correlations, df_clean\n",
    "\n",
    "correlations, clean_df = calculate_comprehensive_correlations(analysis_df)\n",
    "\n",
    "if correlations:\n",
    "    print(f\"\\n=== CORRELATION ANALYSIS RESULTS ===\")\n",
    "    print(f\"Clean dataset shape: {clean_df.shape}\")\n",
    "    \n",
    "    # Create comprehensive correlation heatmaps\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(24, 8))\n",
    "    \n",
    "    methods = ['pearson', 'spearman', 'kendall']\n",
    "    \n",
    "    for i, method in enumerate(methods):\n",
    "        mask = np.triu(np.ones_like(correlations[method], dtype=bool))\n",
    "        sns.heatmap(correlations[method], \n",
    "                   mask=mask,\n",
    "                   annot=True, \n",
    "                   cmap='RdYlBu_r', \n",
    "                   center=0,\n",
    "                   fmt='.2f',\n",
    "                   square=True,\n",
    "                   ax=axes[i],\n",
    "                   cbar_kws={'shrink': 0.8})\n",
    "        axes[i].set_title(f'{method.capitalize()} Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "        axes[i].tick_params(axis='x', rotation=45)\n",
    "        axes[i].tick_params(axis='y', rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/correlation_matrices.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Insufficient data for correlation analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Identification of Unusual Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify statistically significant and unusual correlations\n",
    "def find_unusual_correlations(correlations_dict, threshold=0.5, significance_level=0.05):\n",
    "    \"\"\"\n",
    "    Find correlations that are statistically significant and above threshold\n",
    "    \"\"\"\n",
    "    unusual_correlations = []\n",
    "    \n",
    "    if not correlations_dict:\n",
    "        return unusual_correlations\n",
    "    \n",
    "    for method, corr_matrix in correlations_dict.items():\n",
    "        # Get correlation pairs\n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i+1, len(corr_matrix.columns)):\n",
    "                var1 = corr_matrix.columns[i]\n",
    "                var2 = corr_matrix.columns[j]\n",
    "                correlation = corr_matrix.iloc[i, j]\n",
    "                \n",
    "                # Skip if NaN\n",
    "                if pd.isna(correlation):\n",
    "                    continue\n",
    "                \n",
    "                # Check if correlation is above threshold\n",
    "                if abs(correlation) >= threshold:\n",
    "                    # Calculate p-value for significance testing\n",
    "                    if len(clean_df) > 2:\n",
    "                        if method == 'pearson':\n",
    "                            _, p_value = pearsonr(clean_df[var1].dropna(), clean_df[var2].dropna())\n",
    "                        elif method == 'spearman':\n",
    "                            _, p_value = spearmanr(clean_df[var1].dropna(), clean_df[var2].dropna())\n",
    "                        elif method == 'kendall':\n",
    "                            _, p_value = kendalltau(clean_df[var1].dropna(), clean_df[var2].dropna())\n",
    "                        \n",
    "                        if p_value < significance_level:\n",
    "                            unusual_correlations.append({\n",
    "                                'method': method,\n",
    "                                'variable_1': var1,\n",
    "                                'variable_2': var2,\n",
    "                                'correlation': correlation,\n",
    "                                'p_value': p_value,\n",
    "                                'significance': 'Highly Significant' if p_value < 0.01 else 'Significant',\n",
    "                                'strength': 'Strong' if abs(correlation) > 0.7 else 'Moderate'\n",
    "                            })\n",
    "    \n",
    "    return unusual_correlations\n",
    "\n",
    "unusual_corrs = find_unusual_correlations(correlations, threshold=0.4, significance_level=0.05)\n",
    "\n",
    "print(\"\\n=== UNUSUAL CORRELATIONS DETECTED ===\")\n",
    "print(f\"Total significant correlations found: {len(unusual_corrs)}\")\n",
    "\n",
    "if unusual_corrs:\n",
    "    unusual_df = pd.DataFrame(unusual_corrs)\n",
    "    unusual_df = unusual_df.sort_values('correlation', key=abs, ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 Strongest Unusual Correlations:\")\n",
    "    print(\"=\" * 100)\n",
    "    for i, row in unusual_df.head(10).iterrows():\n",
    "        print(f\"{row['variable_1']} â†” {row['variable_2']}\")\n",
    "        print(f\"  Method: {row['method']} | Correlation: {row['correlation']:.3f} | p-value: {row['p_value']:.2e}\")\n",
    "        print(f\"  Strength: {row['strength']} | Significance: {row['significance']}\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    # Save unusual correlations\n",
    "    unusual_df.to_csv('../results/unusual_correlations.csv', index=False)\n",
    "else:\n",
    "    print(\"No significant unusual correlations detected with current thresholds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Network Analysis of Variable Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create network graph of significant correlations\n",
    "def create_correlation_network(unusual_correlations, min_correlation=0.5):\n",
    "    \"\"\"\n",
    "    Create network graph showing variable relationships\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add edges for significant correlations\n",
    "    for corr in unusual_correlations:\n",
    "        if abs(corr['correlation']) >= min_correlation:\n",
    "            G.add_edge(corr['variable_1'], corr['variable_2'], \n",
    "                      weight=abs(corr['correlation']),\n",
    "                      correlation=corr['correlation'],\n",
    "                      p_value=corr['p_value'],\n",
    "                      method=corr['method'])\n",
    "    \n",
    "    return G\n",
    "\n",
    "if unusual_corrs:\n",
    "    # Create network\n",
    "    network = create_correlation_network(unusual_corrs, min_correlation=0.4)\n",
    "    \n",
    "    if len(network.nodes()) > 0:\n",
    "        print(f\"\\n=== CORRELATION NETWORK ANALYSIS ===\")\n",
    "        print(f\"Network nodes: {len(network.nodes())}\")\n",
    "        print(f\"Network edges: {len(network.edges())}\")\n",
    "        \n",
    "        # Calculate network metrics\n",
    "        if len(network.nodes()) > 1:\n",
    "            centrality = nx.degree_centrality(network)\n",
    "            betweenness = nx.betweenness_centrality(network)\n",
    "            \n",
    "            print(\"\\nMost connected variables (highest degree centrality):\")\n",
    "            for var, cent in sorted(centrality.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "                print(f\"  {var}: {cent:.3f}\")\n",
    "        \n",
    "        # Visualize network\n",
    "        plt.figure(figsize=(15, 12))\n",
    "        \n",
    "        # Calculate layout\n",
    "        if len(network.nodes()) > 1:\n",
    "            pos = nx.spring_layout(network, k=3, iterations=50)\n",
    "            \n",
    "            # Draw edges with thickness based on correlation strength\n",
    "            edges = network.edges(data=True)\n",
    "            edge_weights = [edge[2]['weight'] * 5 for edge in edges]\n",
    "            edge_colors = ['red' if edge[2]['correlation'] < 0 else 'blue' for edge in edges]\n",
    "            \n",
    "            nx.draw_networkx_edges(network, pos, width=edge_weights, alpha=0.6, edge_color=edge_colors)\n",
    "            \n",
    "            # Draw nodes\n",
    "            nx.draw_networkx_nodes(network, pos, node_color='lightblue', \n",
    "                                 node_size=1000, alpha=0.8)\n",
    "            \n",
    "            # Draw labels\n",
    "            nx.draw_networkx_labels(network, pos, font_size=8, font_weight='bold')\n",
    "            \n",
    "            plt.title('Variable Correlation Network\\n(Blue=Positive, Red=Negative, Thickness=Strength)', \n",
    "                     fontsize=16, fontweight='bold')\n",
    "            plt.axis('off')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('../results/correlation_network.png', dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(\"Insufficient connections for network analysis\")\n",
    "else:\n",
    "    print(\"No correlations available for network analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Principal Component Analysis for Pattern Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA to identify underlying patterns\n",
    "if len(clean_df) > 2 and len(clean_df.columns) > 2:\n",
    "    print(\"\\n=== PRINCIPAL COMPONENT ANALYSIS ===\")\n",
    "    \n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(clean_df)\n",
    "    \n",
    "    # Perform PCA\n",
    "    pca = PCA()\n",
    "    pca_data = pca.fit_transform(scaled_data)\n",
    "    \n",
    "    # Calculate cumulative explained variance\n",
    "    cumsum_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "    \n",
    "    print(f\"First 5 components explain {cumsum_variance[4]:.1%} of variance\")\n",
    "    \n",
    "    # Create comprehensive PCA visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Scree plot\n",
    "    axes[0,0].plot(range(1, len(pca.explained_variance_ratio_) + 1), \n",
    "                   pca.explained_variance_ratio_, 'bo-', linewidth=2, markersize=8)\n",
    "    axes[0,0].set_xlabel('Principal Component')\n",
    "    axes[0,0].set_ylabel('Explained Variance Ratio')\n",
    "    axes[0,0].set_title('Scree Plot')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Cumulative variance\n",
    "    axes[0,1].plot(range(1, len(cumsum_variance) + 1), cumsum_variance, 'ro-', linewidth=2, markersize=8)\n",
    "    axes[0,1].axhline(y=0.8, color='k', linestyle='--', alpha=0.5, label='80% threshold')\n",
    "    axes[0,1].axhline(y=0.95, color='k', linestyle='--', alpha=0.5, label='95% threshold')\n",
    "    axes[0,1].set_xlabel('Number of Components')\n",
    "    axes[0,1].set_ylabel('Cumulative Explained Variance')\n",
    "    axes[0,1].set_title('Cumulative Explained Variance')\n",
    "    axes[0,1].legend()\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. PC1 vs PC2 scatter plot\n",
    "    if len(pca_data) > 1:\n",
    "        scatter = axes[1,0].scatter(pca_data[:, 0], pca_data[:, 1], \n",
    "                                   c=range(len(pca_data)), cmap='viridis', alpha=0.7, s=100)\n",
    "        axes[1,0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "        axes[1,0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "        axes[1,0].set_title('PC1 vs PC2 Scatter Plot')\n",
    "        axes[1,0].grid(True, alpha=0.3)\n",
    "        plt.colorbar(scatter, ax=axes[1,0], label='Sample Index')\n",
    "    \n",
    "    # 4. Loading plot for PC1 and PC2\n",
    "    loadings = pca.components_[:2].T  # First two components\n",
    "    feature_names = clean_df.columns\n",
    "    \n",
    "    for i, (feature, loading) in enumerate(zip(feature_names, loadings)):\n",
    "        axes[1,1].arrow(0, 0, loading[0], loading[1], \n",
    "                       head_width=0.05, head_length=0.05, fc='red', ec='red', alpha=0.7)\n",
    "        axes[1,1].text(loading[0]*1.1, loading[1]*1.1, feature, \n",
    "                      fontsize=8, ha='center', va='center')\n",
    "    \n",
    "    axes[1,1].set_xlabel(f'PC1 Loading ({pca.explained_variance_ratio_[0]:.1%})')\n",
    "    axes[1,1].set_ylabel(f'PC2 Loading ({pca.explained_variance_ratio_[1]:.1%})')\n",
    "    axes[1,1].set_title('PCA Loading Plot')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    axes[1,1].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "    axes[1,1].axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/pca_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Identify most important features for each component\n",
    "    print(\"\\nMost important features by component:\")\n",
    "    for i in range(min(3, len(pca.components_))):\n",
    "        component_loadings = list(zip(feature_names, pca.components_[i]))\n",
    "        component_loadings.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "        \n",
    "        print(f\"\\nPC{i+1} (explains {pca.explained_variance_ratio_[i]:.1%} variance):\")\n",
    "        for feature, loading in component_loadings[:5]:\n",
    "            print(f\"  {feature}: {loading:.3f}\")\n",
    "else:\n",
    "    print(\"Insufficient data for PCA analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Anomaly Detection Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple anomaly detection methods\n",
    "if len(clean_df) > 3:\n",
    "    print(\"\\n=== ANOMALY DETECTION ANALYSIS ===\")\n",
    "    \n",
    "    # Standardize data for anomaly detection\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(clean_df)\n",
    "    \n",
    "    anomaly_results = {}\n",
    "    \n",
    "    # 1. Isolation Forest\n",
    "    iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "    iso_anomalies = iso_forest.fit_predict(scaled_data)\n",
    "    anomaly_results['isolation_forest'] = iso_anomalies\n",
    "    \n",
    "    # 2. Elliptic Envelope (for multivariate normal distributions)\n",
    "    if scaled_data.shape[1] > 1:\n",
    "        elliptic = EllipticEnvelope(contamination=0.1, random_state=42)\n",
    "        elliptic_anomalies = elliptic.fit_predict(scaled_data)\n",
    "        anomaly_results['elliptic_envelope'] = elliptic_anomalies\n",
    "    \n",
    "    # 3. Statistical outliers (Z-score method)\n",
    "    z_scores = np.abs(stats.zscore(scaled_data, axis=0))\n",
    "    z_anomalies = np.where(np.any(z_scores > 3, axis=1), -1, 1)\n",
    "    anomaly_results['z_score'] = z_anomalies\n",
    "    \n",
    "    # Combine results\n",
    "    anomaly_df = pd.DataFrame(anomaly_results)\n",
    "    anomaly_df['anomaly_count'] = (anomaly_df == -1).sum(axis=1)\n",
    "    anomaly_df['is_outlier'] = anomaly_df['anomaly_count'] >= 2  # Consensus outlier\n",
    "    \n",
    "    print(f\"Anomalies detected by different methods:\")\n",
    "    for method in anomaly_results.keys():\n",
    "        n_anomalies = (anomaly_results[method] == -1).sum()\n",
    "        print(f\"  {method}: {n_anomalies} anomalies ({n_anomalies/len(clean_df)*100:.1f}%)\")\n",
    "    \n",
    "    consensus_outliers = anomaly_df['is_outlier'].sum()\n",
    "    print(f\"\\nConsensus outliers (detected by â‰¥2 methods): {consensus_outliers}\")\n",
    "    \n",
    "    # Visualize anomalies\n",
    "    if len(pca_data) > 1:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        methods_to_plot = list(anomaly_results.keys())[:3] + ['consensus']\n",
    "        \n",
    "        for i, method in enumerate(methods_to_plot):\n",
    "            row, col = i // 2, i % 2\n",
    "            \n",
    "            if method == 'consensus':\n",
    "                colors = ['red' if outlier else 'blue' for outlier in anomaly_df['is_outlier']]\n",
    "                title = 'Consensus Outliers (â‰¥2 methods)'\n",
    "            else:\n",
    "                colors = ['red' if x == -1 else 'blue' for x in anomaly_results[method]]\n",
    "                title = f'{method.replace(\"_\", \" \").title()} Outliers'\n",
    "            \n",
    "            scatter = axes[row, col].scatter(pca_data[:, 0], pca_data[:, 1], \n",
    "                                           c=colors, alpha=0.7, s=100)\n",
    "            axes[row, col].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\n",
    "            axes[row, col].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\n",
    "            axes[row, col].set_title(title)\n",
    "            axes[row, col].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('../results/anomaly_detection.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    # Identify characteristics of outliers\n",
    "    if consensus_outliers > 0:\n",
    "        outlier_indices = anomaly_df[anomaly_df['is_outlier']].index\n",
    "        print(\"\\n=== OUTLIER CHARACTERISTICS ===\")\n",
    "        \n",
    "        outlier_data = df.iloc[outlier_indices]\n",
    "        \n",
    "        print(\"Outlier trials:\")\n",
    "        for idx in outlier_indices:\n",
    "            trial = df.iloc[idx]\n",
    "            print(f\"  Trial {idx}: {trial['trial_id']} - {trial['condition']} - {trial['intervention']}\")\n",
    "            print(f\"    Endpoint: {trial['endpoint_value']}, N: {trial['n_patients']}, Duration: {trial['follow_up_months']}\")\n",
    "        \n",
    "        # Statistical comparison of outliers vs normal trials\n",
    "        normal_indices = anomaly_df[~anomaly_df['is_outlier']].index\n",
    "        normal_data = df.iloc[normal_indices]\n",
    "        \n",
    "        print(\"\\nOutlier vs Normal Trial Comparison:\")\n",
    "        for col in ['endpoint_value', 'n_patients', 'follow_up_months', 'safety_events']:\n",
    "            if col in df.columns:\n",
    "                outlier_values = outlier_data[col].dropna()\n",
    "                normal_values = normal_data[col].dropna()\n",
    "                \n",
    "                if len(outlier_values) > 0 and len(normal_values) > 0:\n",
    "                    outlier_mean = outlier_values.mean()\n",
    "                    normal_mean = normal_values.mean()\n",
    "                    print(f\"  {col}: Outliers={outlier_mean:.2f}, Normal={normal_mean:.2f}, Diff={outlier_mean-normal_mean:.2f}\")\n",
    "else:\n",
    "    print(\"Insufficient data for anomaly detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hierarchical Clustering for Pattern Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical clustering to identify trial groupings\n",
    "if len(clean_df) > 3:\n",
    "    print(\"\\n=== HIERARCHICAL CLUSTERING ANALYSIS ===\")\n",
    "    \n",
    "    # Perform hierarchical clustering\n",
    "    linkage_matrix = linkage(scaled_data, method='ward')\n",
    "    \n",
    "    # Create dendrogram\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Get trial labels for dendrogram\n",
    "    trial_labels = [f\"{df.iloc[i]['trial_id']}_{df.iloc[i]['condition'][:3]}\" \n",
    "                   for i in clean_df.index]\n",
    "    \n",
    "    dendrogram(linkage_matrix, \n",
    "               labels=trial_labels,\n",
    "               leaf_rotation=90,\n",
    "               leaf_font_size=8)\n",
    "    \n",
    "    plt.title('Hierarchical Clustering of Clinical Trials\\n(Based on All Numerical Features)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Trials')\n",
    "    plt.ylabel('Distance')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/hierarchical_clustering.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Form clusters\n",
    "    n_clusters = 3  # Optimal number can be determined by elbow method\n",
    "    cluster_labels = fcluster(linkage_matrix, n_clusters, criterion='maxclust')\n",
    "    \n",
    "    # Analyze cluster characteristics\n",
    "    cluster_df = df.iloc[clean_df.index].copy()\n",
    "    cluster_df['cluster'] = cluster_labels\n",
    "    \n",
    "    print(f\"\\nIdentified {n_clusters} clusters:\")\n",
    "    for cluster_id in range(1, n_clusters + 1):\n",
    "        cluster_trials = cluster_df[cluster_df['cluster'] == cluster_id]\n",
    "        print(f\"\\nCluster {cluster_id} ({len(cluster_trials)} trials):\")\n",
    "        \n",
    "        # Dominant characteristics\n",
    "        condition_counts = cluster_trials['condition'].value_counts()\n",
    "        intervention_counts = cluster_trials['intervention'].value_counts()\n",
    "        \n",
    "        print(f\"  Conditions: {dict(condition_counts)}\")\n",
    "        print(f\"  Interventions: {dict(intervention_counts.head(2))}\")\n",
    "        \n",
    "        # Statistical summary\n",
    "        if 'endpoint_value' in cluster_trials.columns:\n",
    "            endpoint_stats = cluster_trials['endpoint_value'].describe()\n",
    "            print(f\"  Endpoint value: mean={endpoint_stats['mean']:.1f}, std={endpoint_stats['std']:.1f}\")\n",
    "        \n",
    "        if 'n_patients' in cluster_trials.columns:\n",
    "            patient_stats = cluster_trials['n_patients'].describe()\n",
    "            print(f\"  Sample size: mean={patient_stats['mean']:.0f}, range={patient_stats['min']:.0f}-{patient_stats['max']:.0f}\")\n",
    "    \n",
    "    # Visualize clusters in PCA space\n",
    "    if len(pca_data) > 1:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        scatter = plt.scatter(pca_data[:, 0], pca_data[:, 1], \n",
    "                            c=cluster_labels, cmap='Set1', s=100, alpha=0.7)\n",
    "        plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "        plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "        plt.title('Trial Clusters in Principal Component Space')\n",
    "        plt.colorbar(scatter, label='Cluster')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add trial labels\n",
    "        for i, (x, y) in enumerate(pca_data[:, :2]):\n",
    "            trial_id = df.iloc[clean_df.index[i]]['trial_id']\n",
    "            plt.annotate(trial_id[:8], (x, y), xytext=(5, 5), \n",
    "                        textcoords='offset points', fontsize=6, alpha=0.7)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('../results/cluster_pca_visualization.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\nelse:\n",
    "    print(\"Insufficient data for hierarchical clustering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Time Series and Temporal Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze temporal patterns and follow-up duration effects\n",
    "print(\"\\n=== TEMPORAL PATTERN ANALYSIS ===\")\n",
    "\n",
    "# Extract trials with temporal data\n",
    "temporal_df = df[df['follow_up_months'].notna() & df['endpoint_value'].notna()].copy()\n",
    "\n",
    "if len(temporal_df) > 3:\n",
    "    # Create comprehensive temporal visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Efficacy vs Follow-up Duration\n",
    "    conditions = temporal_df['condition'].unique()\n",
    "    colors = plt.cm.Set1(np.linspace(0, 1, len(conditions)))\n",
    "    \n",
    "    for i, condition in enumerate(conditions):\n",
    "        condition_data = temporal_df[temporal_df['condition'] == condition]\n",
    "        axes[0,0].scatter(condition_data['follow_up_months'], \n",
    "                         condition_data['endpoint_value'],\n",
    "                         label=condition, color=colors[i], s=100, alpha=0.7)\n",
    "    \n",
    "    # Add trend line\n",
    "    if len(temporal_df) > 1:\n",
    "        z = np.polyfit(temporal_df['follow_up_months'], temporal_df['endpoint_value'], 1)\n",
    "        p = np.poly1d(z)\n",
    "        axes[0,0].plot(temporal_df['follow_up_months'], p(temporal_df['follow_up_months']), \n",
    "                      \"r--\", alpha=0.8, linewidth=2, label=f'Trend (slope={z[0]:.2f})')\n",
    "    \n",
    "    axes[0,0].set_xlabel('Follow-up Duration (months)')\n",
    "    axes[0,0].set_ylabel('Endpoint Value')\n",
    "    axes[0,0].set_title('Efficacy vs Follow-up Duration by Condition')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Sample Size vs Follow-up Duration\n",
    "    scatter = axes[0,1].scatter(temporal_df['follow_up_months'], \n",
    "                               temporal_df['n_patients'],\n",
    "                               c=temporal_df['endpoint_value'], \n",
    "                               cmap='viridis', s=100, alpha=0.7)\n",
    "    axes[0,1].set_xlabel('Follow-up Duration (months)')\n",
    "    axes[0,1].set_ylabel('Number of Patients')\n",
    "    axes[0,1].set_title('Sample Size vs Follow-up (colored by efficacy)')\n",
    "    plt.colorbar(scatter, ax=axes[0,1], label='Endpoint Value')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Follow-up Duration Distribution by Condition\n",
    "    temporal_df.boxplot(column='follow_up_months', by='condition', ax=axes[1,0])\n",
    "    axes[1,0].set_title('Follow-up Duration Distribution by Condition')\n",
    "    axes[1,0].set_xlabel('Condition')\n",
    "    axes[1,0].set_ylabel('Follow-up Duration (months)')\n",
    "    \n",
    "    # 4. Efficacy Distribution by Follow-up Bins\n",
    "    temporal_df['followup_bin'] = pd.cut(temporal_df['follow_up_months'], \n",
    "                                        bins=[0, 6, 12, 18, 24, 100], \n",
    "                                        labels=['â‰¤6m', '6-12m', '12-18m', '18-24m', '>24m'])\n",
    "    \n",
    "    followup_efficacy = temporal_df.groupby('followup_bin')['endpoint_value'].agg(['mean', 'std', 'count'])\n",
    "    \n",
    "    x_pos = range(len(followup_efficacy))\n",
    "    axes[1,1].bar(x_pos, followup_efficacy['mean'], \n",
    "                  yerr=followup_efficacy['std'], capsize=5, alpha=0.7)\n",
    "    axes[1,1].set_xlabel('Follow-up Duration Bins')\n",
    "    axes[1,1].set_ylabel('Mean Endpoint Value')\n",
    "    axes[1,1].set_title('Efficacy by Follow-up Duration Bins')\n",
    "    axes[1,1].set_xticks(x_pos)\n",
    "    axes[1,1].set_xticklabels(followup_efficacy.index)\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add sample sizes on bars\n",
    "    for i, (count, mean_val) in enumerate(zip(followup_efficacy['count'], followup_efficacy['mean'])):\n",
    "        axes[1,1].text(i, mean_val + 2, f'n={count}', ha='center', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/temporal_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistical analysis of temporal effects\n",
    "    correlation_followup_efficacy = stats.pearsonr(temporal_df['follow_up_months'], \n",
    "                                                  temporal_df['endpoint_value'])\n",
    "    \n",
    "    print(f\"\\nTemporal Pattern Statistics:\")\n",
    "    print(f\"Follow-up vs Efficacy correlation: r={correlation_followup_efficacy[0]:.3f}, p={correlation_followup_efficacy[1]:.3f}\")\n",
    "    print(f\"Interpretation: {'Significant positive trend' if correlation_followup_efficacy[1] < 0.05 and correlation_followup_efficacy[0] > 0 else 'No significant temporal trend'}\")\n",
    "    \n",
    "    # Analyze by duration bins\n",
    "    print(f\"\\nEfficacy by follow-up duration:\")\n",
    "    for duration_bin, group_data in followup_efficacy.iterrows():\n",
    "        if pd.notna(group_data['mean']):\n",
    "            print(f\"  {duration_bin}: mean={group_data['mean']:.1f}Â±{group_data['std']:.1f} (n={group_data['count']})\")\nelse:\n",
    "    print(\"Insufficient temporal data for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Advanced Statistical Pattern Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced pattern detection using multiple statistical approaches\n",
    "print(\"\\n=== ADVANCED STATISTICAL PATTERN DETECTION ===\")\n",
    "\n",
    "# 1. Non-linear correlation detection\n",
    "def calculate_mutual_information(x, y, bins=5):\n",
    "    \"\"\"\n",
    "    Calculate mutual information between two variables\n",
    "    \"\"\"\n",
    "    from scipy.stats import entropy\n",
    "    \n",
    "    # Discretize continuous variables\n",
    "    x_binned = pd.cut(x, bins=bins, labels=False)\n",
    "    y_binned = pd.cut(y, bins=bins, labels=False)\n",
    "    \n",
    "    # Calculate joint and marginal distributions\n",
    "    joint_dist = pd.crosstab(x_binned, y_binned, normalize=True)\n",
    "    x_dist = pd.Series(x_binned).value_counts(normalize=True)\n",
    "    y_dist = pd.Series(y_binned).value_counts(normalize=True)\n",
    "    \n",
    "    # Calculate mutual information\n",
    "    mi = 0\n",
    "    for i in joint_dist.index:\n",
    "        for j in joint_dist.columns:\n",
    "            if joint_dist.loc[i, j] > 0:\n",
    "                mi += joint_dist.loc[i, j] * np.log2(joint_dist.loc[i, j] / (x_dist[i] * y_dist[j]))\n",
    "    \n",
    "    return mi\n",
    "\n",
    "# Calculate mutual information for all variable pairs\n",
    "if len(clean_df) > 5:\n",
    "    mi_matrix = pd.DataFrame(index=clean_df.columns, columns=clean_df.columns)\n",
    "    \n",
    "    for col1 in clean_df.columns:\n",
    "        for col2 in clean_df.columns:\n",
    "            if col1 != col2:\n",
    "                try:\n",
    "                    mi = calculate_mutual_information(clean_df[col1].dropna(), \n",
    "                                                    clean_df[col2].dropna())\n",
    "                    mi_matrix.loc[col1, col2] = mi\n",
    "                except:\n",
    "                    mi_matrix.loc[col1, col2] = np.nan\n",
    "            else:\n",
    "                mi_matrix.loc[col1, col2] = 0\n",
    "    \n",
    "    # Convert to numeric\n",
    "    mi_matrix = mi_matrix.astype(float)\n",
    "    \n",
    "    # Find highest mutual information pairs\n",
    "    mi_pairs = []\n",
    "    for i in range(len(mi_matrix.columns)):\n",
    "        for j in range(i+1, len(mi_matrix.columns)):\n",
    "            var1 = mi_matrix.columns[i]\n",
    "            var2 = mi_matrix.columns[j]\n",
    "            mi_value = mi_matrix.iloc[i, j]\n",
    "            if pd.notna(mi_value) and mi_value > 0.1:\n",
    "                mi_pairs.append((var1, var2, mi_value))\n",
    "    \n",
    "    mi_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    print(\"High Mutual Information Pairs (non-linear relationships):\")\n",
    "    for var1, var2, mi_val in mi_pairs[:10]:\n",
    "        print(f\"  {var1} â†” {var2}: MI = {mi_val:.3f}\")\n",
    "    \n",
    "    # Visualize mutual information matrix\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    mask = np.triu(np.ones_like(mi_matrix, dtype=bool))\n",
    "    sns.heatmap(mi_matrix, mask=mask, annot=True, cmap='YlOrRd', \n",
    "                center=0, fmt='.2f', square=True)\n",
    "    plt.title('Mutual Information Matrix\\n(Detects Non-linear Relationships)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/mutual_information_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\nelse:\n",
    "    print(\"Insufficient data for mutual information analysis\")\n",
    "\n",
    "# 2. Kolmogorov-Smirnov tests for distribution differences\n",
    "print(\"\\n=== DISTRIBUTION DIFFERENCE ANALYSIS ===\")\n",
    "\n",
    "if len(df) > 5:\n",
    "    # Compare distributions between conditions\n",
    "    from scipy.stats import ks_2samp\n",
    "    \n",
    "    conditions = df['condition'].unique()\n",
    "    numerical_cols = ['endpoint_value', 'n_patients', 'follow_up_months', 'safety_events']\n",
    "    \n",
    "    ks_results = []\n",
    "    \n",
    "    for col in numerical_cols:\n",
    "        if col in df.columns:\n",
    "            col_data = df[col].dropna()\n",
    "            if len(col_data) > 3:\n",
    "                for i, cond1 in enumerate(conditions):\n",
    "                    for cond2 in conditions[i+1:]:\n",
    "                        data1 = df[df['condition'] == cond1][col].dropna()\n",
    "                        data2 = df[df['condition'] == cond2][col].dropna()\n",
    "                        \n",
    "                        if len(data1) > 1 and len(data2) > 1:\n",
    "                            ks_stat, ks_p = ks_2samp(data1, data2)\n",
    "                            ks_results.append({\n",
    "                                'variable': col,\n",
    "                                'condition_1': cond1,\n",
    "                                'condition_2': cond2,\n",
    "                                'ks_statistic': ks_stat,\n",
    "                                'p_value': ks_p,\n",
    "                                'significant': ks_p < 0.05\n",
    "                            })\n",
    "    \n",
    "    if ks_results:\n",
    "        ks_df = pd.DataFrame(ks_results)\n",
    "        significant_diffs = ks_df[ks_df['significant']]\n",
    "        \n",
    "        print(\"Significant distribution differences between conditions:\")\n",
    "        for _, row in significant_diffs.iterrows():\n",
    "            print(f\"  {row['variable']}: {row['condition_1']} vs {row['condition_2']}\")\n",
    "            print(f\"    KS statistic: {row['ks_statistic']:.3f}, p-value: {row['p_value']:.3e}\")\n",
    "    else:\n",
    "        print(\"No significant distribution differences detected\")\nelse:\n",
    "    print(\"Insufficient data for distribution analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary of Unusual Patterns and Significant Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive summary of all detected patterns\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE ANALYSIS SUMMARY - UNUSUAL PATTERNS DETECTED\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary_findings = {\n",
    "    'significant_correlations': [],\n",
    "    'anomalous_trials': [],\n",
    "    'cluster_patterns': [],\n",
    "    'temporal_trends': [],\n",
    "    'distribution_differences': []\n",
    "}\n",
    "\n",
    "# 1. Significant Correlations Summary\n",
    "print(\"\\n1. SIGNIFICANT CORRELATIONS (p < 0.05, |r| > 0.4):\")\n",
    "if unusual_corrs:\n",
    "    for corr in unusual_corrs[:5]:  # Top 5\n",
    "        print(f\"   â€¢ {corr['variable_1']} â†” {corr['variable_2']}: r={corr['correlation']:.3f} (p={corr['p_value']:.2e})\")\n",
    "        summary_findings['significant_correlations'].append(corr)\n",
    "else:\n",
    "    print(\"   â€¢ No significant correlations above threshold detected\")\n",
    "\n",
    "# 2. Anomalous Trials Summary\n",
    "print(\"\\n2. ANOMALOUS TRIALS (Consensus outliers):\")\n",
    "if 'anomaly_df' in locals() and consensus_outliers > 0:\n",
    "    outlier_indices = anomaly_df[anomaly_df['is_outlier']].index\n",
    "    for idx in outlier_indices:\n",
    "        trial = df.iloc[idx]\n",
    "        print(f\"   â€¢ {trial['trial_id']}: {trial['condition']} - Endpoint: {trial['endpoint_value']}, N: {trial['n_patients']}\")\n",
    "        summary_findings['anomalous_trials'].append({\n",
    "            'trial_id': trial['trial_id'],\n",
    "            'condition': trial['condition'],\n",
    "            'endpoint_value': trial['endpoint_value'],\n",
    "            'n_patients': trial['n_patients']\n",
    "        })\n",
    "else:\n",
    "    print(\"   â€¢ No consensus anomalous trials detected\")\n",
    "\n",
    "# 3. Cluster Patterns Summary\n",
    "print(\"\\n3. CLUSTER PATTERNS:\")\n",
    "if 'cluster_df' in locals():\n",
    "    for cluster_id in range(1, n_clusters + 1):\n",
    "        cluster_trials = cluster_df[cluster_df['cluster'] == cluster_id]\n",
    "        dominant_condition = cluster_trials['condition'].mode()[0] if not cluster_trials['condition'].mode().empty else 'Mixed'\n",
    "        mean_efficacy = cluster_trials['endpoint_value'].mean()\n",
    "        print(f\"   â€¢ Cluster {cluster_id}: {len(cluster_trials)} trials, dominated by {dominant_condition}, mean efficacy: {mean_efficacy:.1f}\")\n",
    "        summary_findings['cluster_patterns'].append({\n",
    "            'cluster_id': cluster_id,\n",
    "            'size': len(cluster_trials),\n",
    "            'dominant_condition': dominant_condition,\n",
    "            'mean_efficacy': mean_efficacy\n",
    "        })\n",
    "else:\n",
    "    print(\"   â€¢ Clustering analysis not performed\")\n",
    "\n",
    "# 4. Temporal Trends Summary\n",
    "print(\"\\n4. TEMPORAL TRENDS:\")\n",
    "if 'correlation_followup_efficacy' in locals():\n",
    "    trend_direction = \"increasing\" if correlation_followup_efficacy[0] > 0 else \"decreasing\"\n",
    "    significance = \"significant\" if correlation_followup_efficacy[1] < 0.05 else \"non-significant\"\n",
    "    print(f\"   â€¢ Follow-up duration vs efficacy: {trend_direction} trend (r={correlation_followup_efficacy[0]:.3f}, {significance})\")\n",
    "    summary_findings['temporal_trends'].append({\n",
    "        'correlation': correlation_followup_efficacy[0],\n",
    "        'p_value': correlation_followup_efficacy[1],\n",
    "        'trend': trend_direction,\n",
    "        'significance': significance\n",
    "    })\n",
    "else:\n",
    "    print(\"   â€¢ No temporal analysis performed\")\n",
    "\n",
    "# 5. Distribution Differences Summary\n",
    "print(\"\\n5. DISTRIBUTION DIFFERENCES:\")\n",
    "if 'ks_results' in locals() and ks_results:\n",
    "    significant_ks = [r for r in ks_results if r['significant']]\n",
    "    if significant_ks:\n",
    "        for ks in significant_ks[:3]:  # Top 3\n",
    "            print(f\"   â€¢ {ks['variable']}: {ks['condition_1']} vs {ks['condition_2']} (KS={ks['ks_statistic']:.3f}, p={ks['p_value']:.2e})\")\n",
    "            summary_findings['distribution_differences'].append(ks)\n",
    "    else:\n",
    "        print(\"   â€¢ No significant distribution differences detected\")\n",
    "else:\n",
    "    print(\"   â€¢ Distribution analysis not performed\")\n",
    "\n",
    "# 6. Statistical Significance Summary\n",
    "print(\"\\n6. STATISTICAL SIGNIFICANCE SUMMARY:\")\n",
    "total_significant = len(summary_findings['significant_correlations'])\n",
    "total_anomalies = len(summary_findings['anomalous_trials'])\n",
    "total_clusters = len(summary_findings['cluster_patterns'])\n",
    "\n",
    "print(f\"   â€¢ Significant correlations detected: {total_significant}\")\n",
    "print(f\"   â€¢ Anomalous trials identified: {total_anomalies}\")\n",
    "print(f\"   â€¢ Distinct trial clusters: {total_clusters}\")\n",
    "print(f\"   â€¢ Dataset completeness: {len(clean_df)}/{len(df)} trials with complete data\")\n",
    "\n",
    "# Save comprehensive summary\n",
    "import json\n",
    "with open('../results/pattern_analysis_summary.json', 'w') as f:\n",
    "    # Convert numpy types to native Python types for JSON serialization\n",
    "    def convert_numpy(obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return obj\n",
    "    \n",
    "    # Convert all numpy types in summary_findings\n",
    "    json_compatible = json.loads(json.dumps(summary_findings, default=convert_numpy))\n",
    "    json.dump(json_compatible, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS COMPLETE - All results saved to ../results/ directory\")\n",
    "print(\"Generated files:\")\n",
    "print(\"  â€¢ correlation_matrices.png\")\n",
    "print(\"  â€¢ correlation_network.png\")\n",
    "print(\"  â€¢ pca_analysis.png\")\n",
    "print(\"  â€¢ anomaly_detection.png\")\n",
    "print(\"  â€¢ hierarchical_clustering.png\")\n",
    "print(\"  â€¢ temporal_analysis.png\")\n",
    "print(\"  â€¢ mutual_information_matrix.png\")\n",
    "print(\"  â€¢ pattern_analysis_summary.json\")\n",
    "print(\"  â€¢ unusual_correlations.csv\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n",
   "language": "python",\n   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}